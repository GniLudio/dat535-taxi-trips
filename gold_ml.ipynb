{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee2bf62",
   "metadata": {},
   "source": [
    "# Gold - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a020af",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bbb980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "import shared\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9213c652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/28 22:39:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"WARN\".\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from utils import setup_spark\n",
    "\n",
    "# .master(\"spark://group-11:7077\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"DAT535-Taxi-Trips\")\n",
    "    .master(\"spark://group-11:7077\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.log.level\", \"WARN\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb89fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from schemas import schema_merged_silver\n",
    "from shared import PATH_SILVER_MERGED\n",
    "\n",
    "FARE_AMOUNT_MIN, FARE_AMOUNT_MAX = 1, 1000\n",
    "SPEED_MPH_MIN = 1\n",
    "TIP_AMOUNT_MIN, TIP_AMOUNT_MAX = 1, 1000\n",
    "DISTANCE_MILES_MIN, DISTANCE_MILES_MAX = 1, 1000\n",
    "DURATION_MIN, DURATION_MAX = 1, 1000\n",
    "\n",
    "df = spark.read.schema(schema_merged_silver).parquet(PATH_SILVER_MERGED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c7089",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be48e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_regression = df.filter(\n",
    "    (f.col(\"payment_type\") == shared.PaymentType.CREDIT_CARD.value)\n",
    "    & (f.col(\"fare_amount\").between(FARE_AMOUNT_MIN, FARE_AMOUNT_MAX))\n",
    "    & (f.col(\"speed_mph\") >= SPEED_MPH_MIN)\n",
    "    & (f.col(\"tip_amount\").between(TIP_AMOUNT_MIN, TIP_AMOUNT_MAX))\n",
    "    & (f.col(\"distance_miles\").between(DISTANCE_MILES_MIN, DISTANCE_MILES_MAX))\n",
    "    & (f.col(\"duration_minutes\").between(DURATION_MIN, DURATION_MAX))\n",
    ").select(\"city\", \"fare_amount\", \"speed_mph\", \"tip_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbe5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_regression.randomSplit([80.0, 20.0], seed=42)\n",
    "train, test = train.cache(), test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e50f0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = Pipeline(\n",
    "    stages=[\n",
    "        StringIndexer(inputCols=[\"city\"], outputCols=[\"cityIndex\"]),\n",
    "        OneHotEncoder(inputCols=[\"cityIndex\"], outputCols=[\"cityVec\"]),\n",
    "        VectorAssembler(inputCols=[\"cityVec\", \"fare_amount\", \"speed_mph\"], outputCol=\"features\"),\n",
    "        StandardScaler(inputCol=\"features\", outputCol=\"featuresScaler\"),\n",
    "        LinearRegression(\n",
    "            maxIter=50, regParam=0.1, elasticNetParam=0.5, labelCol=\"tip_amount\", featuresCol=\"featuresScaler\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6b25bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = stages.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1800844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "predictions_test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2e9b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.408293317685801"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions.select(\"tip_amount\", \"prediction\").show(50)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"tip_amount\", metricName=\"mse\")\n",
    "\n",
    "# metrics.evaluate(predictions_train)\n",
    "metrics.evaluate(predictions_test)\n",
    "#print(model.stages[-1].coefficients)\n",
    "#model.stages[-1].summary.r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "967501e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        tip_amount|\n",
      "+-------+------------------+\n",
      "|  count|          28100022|\n",
      "|   mean|3.2847423196486036|\n",
      "| stddev| 2.845011419459388|\n",
      "|    min|               1.0|\n",
      "|    25%|              1.76|\n",
      "|    50%|              2.34|\n",
      "|    75%|              3.65|\n",
      "|    max|             888.2|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero-tip fraction: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(corr_tip_fare=0.8374642747611836, corr_tip_speed=0.18070588705355897)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df_regression  \n",
    "\n",
    "# Basic distribution of tips\n",
    "df.select(\"tip_amount\").summary().show()\n",
    "\n",
    "# What fraction of trips have zero tip?\n",
    "zero_frac = df.filter(F.col(\"tip_amount\") == 0).count() / df.count()\n",
    "print(\"zero-tip fraction:\", zero_frac)\n",
    "\n",
    "# Correlation between tip and fare / speed\n",
    "corrs = df.select(\n",
    "    F.corr(\"tip_amount\", \"fare_amount\").alias(\"corr_tip_fare\"),\n",
    "    F.corr(\"tip_amount\", \"speed_mph\").alias(\"corr_tip_speed\"),\n",
    ").collect()[0]\n",
    "print(corrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fdb2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d105ca",
   "metadata": {},
   "source": [
    "#### Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07413883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/28 20:50:22 WARN DAGScheduler: Broadcasting large task binary with size 1542.2 KiB\n",
      "25/11/28 20:52:08 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/11/28 20:54:07 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/11/28 20:55:36 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.11.69: Command exited with code 137\n",
      "25/11/28 20:55:36 WARN TaskSetManager: Lost task 8.0 in stage 61.0 (TID 777) (192.168.11.69 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/11/28 20:55:36 WARN TaskSetManager: Lost task 7.0 in stage 61.0 (TID 776) (192.168.11.69 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/11/28 20:55:36 WARN TaskSetManager: Lost task 6.0 in stage 61.0 (TID 775) (192.168.11.69 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/11/28 20:55:36 WARN TaskSetManager: Lost task 9.0 in stage 61.0 (TID 778) (192.168.11.69 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_8 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_11 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_13 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_7 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_9 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_8 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_3 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_1 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_10 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_15 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_13 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_11 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_14 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_3 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_2 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_14 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_1 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_12 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_7 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_2 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_0 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_3 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_7 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_6 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_4 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_9 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_0 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_11 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_15 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_12 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_15 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_13 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_5 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_9 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_12 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_4 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_6 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_8 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_1 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_6 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_0 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_5 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_14 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_5 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_4 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_58_10 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_10 !\n",
      "25/11/28 20:55:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_2 !\n",
      "25/11/28 20:58:34 WARN TaskSetManager: Lost task 12.0 in stage 61.0 (TID 791) (192.168.11.69 executor 1): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$587/725149626.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\n",
      "25/11/28 20:58:35 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.11.69: Command exited with code 52\n",
      "25/11/28 20:58:35 WARN TaskSetManager: Lost task 5.1 in stage 61.0 (TID 788) (192.168.11.69 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 20:58:35 WARN TaskSetManager: Lost task 2.1 in stage 61.0 (TID 787) (192.168.11.69 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 20:58:35 WARN TaskSetManager: Lost task 11.0 in stage 61.0 (TID 790) (192.168.11.69 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_8 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_9 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_5 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_10 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_11 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_8 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_3 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_1 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_1 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_2 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_6 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_7 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_0 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_3 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_5 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_7 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_6 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_4 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_4 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_9 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_10 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_2 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_0 !\n",
      "25/11/28 20:58:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_11 !\n",
      "25/11/28 21:01:31 WARN TaskSetManager: Lost task 12.1 in stage 61.0 (TID 806) (192.168.11.69 executor 2): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$574/1559268782.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\n",
      "25/11/28 21:01:32 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.11.69: Command exited with code 52\n",
      "25/11/28 21:01:32 WARN TaskSetManager: Lost task 1.2 in stage 61.0 (TID 804) (192.168.11.69 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:01:32 WARN TaskSetManager: Lost task 9.2 in stage 61.0 (TID 803) (192.168.11.69 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:01:32 WARN TaskSetManager: Lost task 15.0 in stage 61.0 (TID 808) (192.168.11.69 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:01:32 WARN TaskSetManager: Lost task 14.0 in stage 61.0 (TID 807) (192.168.11.69 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_8 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_13 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_9 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_13 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_5 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_10 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_11 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_8 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_3 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_2 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_1 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_1 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_6 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_7 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_0 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_3 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_5 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_7 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_6 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_4 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_4 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_9 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_10 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_2 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_0 !\n",
      "25/11/28 21:01:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_11 !\n",
      "25/11/28 21:02:50 WARN TaskSetManager: Lost task 7.3 in stage 61.0 (TID 814) (192.168.11.69 executor 3): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$561/1386562066.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\n",
      "25/11/28 21:02:50 ERROR TaskSchedulerImpl: Lost executor 3 on 192.168.11.69: Command exited with code 52\n",
      "25/11/28 21:02:50 WARN TaskSetManager: Lost task 6.3 in stage 61.0 (TID 813) (192.168.11.69 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:02:50 WARN TaskSetManager: Lost task 1.3 in stage 61.0 (TID 812) (192.168.11.69 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:02:50 WARN TaskSetManager: Lost task 13.2 in stage 61.0 (TID 815) (192.168.11.69 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:02:50 WARN TaskSetManager: Lost task 9.3 in stage 61.0 (TID 811) (192.168.11.69 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_15 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_9 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_14 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_6 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_15 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_14 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_1 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_1 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_9 !\n",
      "25/11/28 21:02:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_6 !\n",
      "25/11/28 21:04:22 WARN TaskSetManager: Lost task 8.3 in stage 61.0 (TID 825) (192.168.11.69 executor 4): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:106)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:119)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.hasNext(InMemoryRelation.scala:286)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1601)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$575/2040669707.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1528)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1592)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:326)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\n",
      "25/11/28 21:04:22 ERROR TaskSchedulerImpl: Lost executor 4 on 192.168.11.69: Command exited with code 52\n",
      "25/11/28 21:04:22 WARN TaskSetManager: Lost task 7.4 in stage 61.0 (TID 822) (192.168.11.69 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:04:22 WARN TaskSetManager: Lost task 15.2 in stage 61.0 (TID 821) (192.168.11.69 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:04:22 WARN TaskSetManager: Lost task 11.2 in stage 61.0 (TID 824) (192.168.11.69 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:04:22 WARN TaskSetManager: Lost task 2.3 in stage 61.0 (TID 826) (192.168.11.69 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_7 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_15 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_3 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_13 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_9 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_13 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_7 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_6 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_14 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_15 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_11 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_14 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_3 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_1 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_166_1 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_9 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_6 !\n",
      "25/11/28 21:04:22 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_4_11 !\n",
      "25/11/28 21:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1393.8 KiB\n",
      "25/11/28 21:07:11 WARN DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "25/11/28 21:11:10 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "[Stage 65:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+---------+--------------------+------------------+\n",
      "|   city|fare_amount|speed_mph|tip_amount|cityIndex|            features|        prediction|\n",
      "+-------+-----------+---------+----------+---------+--------------------+------------------+\n",
      "|Chicago|       3.25|      6.0|       2.0|      1.0|      [1.0,3.25,6.0]|2.6256583083349265|\n",
      "|Chicago|       3.25|     8.25|       2.0|      1.0|     [1.0,3.25,8.25]| 2.279885332968586|\n",
      "|Chicago|       3.25|      9.5|      8.25|      1.0|      [1.0,3.25,9.5]|2.1765616285716662|\n",
      "|Chicago|       3.25|10.285715|       3.0|      1.0|[1.0,3.25,10.2857...|2.0119863039988495|\n",
      "|Chicago|       3.25|     12.0|       1.0|      1.0|     [1.0,3.25,12.0]|2.0116219029323816|\n",
      "|Chicago|       3.25|12.900001|       3.0|      1.0|[1.0,3.25,12.9000...|2.0131405727776297|\n",
      "|Chicago|       3.25|14.416666|       3.0|      1.0|[1.0,3.25,14.4166...| 2.018669171744324|\n",
      "|Chicago|       3.25|21.795918|       5.0|      1.0|[1.0,3.25,21.7959...| 2.058532139439839|\n",
      "|Chicago|       3.25|     60.0|       3.0|      1.0|     [1.0,3.25,60.0]| 2.249744447373014|\n",
      "|Chicago|       3.25|     60.0|       4.0|      1.0|     [1.0,3.25,60.0]| 2.249744447373014|\n",
      "|Chicago|       3.25|     84.0|       1.5|      1.0|     [1.0,3.25,84.0]| 2.249744447373014|\n",
      "|Chicago|       3.25|     84.0|      14.6|      1.0|     [1.0,3.25,84.0]| 2.249744447373014|\n",
      "|Chicago|       3.25|    102.0|       3.0|      1.0|    [1.0,3.25,102.0]| 2.249744447373014|\n",
      "|Chicago|       3.25|    342.0|      3.45|      1.0|    [1.0,3.25,342.0]| 2.249744447373014|\n",
      "|Chicago|       3.25|   5226.0|     10.15|      1.0|   [1.0,3.25,5226.0]| 2.249744447373014|\n",
      "|Chicago|       3.75|    138.0|       2.0|      1.0|    [1.0,3.75,138.0]| 2.249744447373014|\n",
      "|Chicago|        4.0|     33.0|       2.0|      1.0|      [1.0,4.0,33.0]| 2.190999917009725|\n",
      "|Chicago|       4.25|     22.0|       2.0|      1.0|     [1.0,4.25,22.0]| 2.058532139439839|\n",
      "|Chicago|       4.25|     39.0|       2.0|      1.0|     [1.0,4.25,39.0]| 2.249744447373014|\n",
      "|Chicago|       4.25|     46.0|       3.0|      1.0|     [1.0,4.25,46.0]| 2.249744447373014|\n",
      "+-------+-----------+---------+----------+---------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from shared import City\n",
    "\n",
    "stages = Pipeline(\n",
    "    stages=[\n",
    "        StringIndexer(\n",
    "            inputCols=[\"city\"],\n",
    "            outputCols=[\"cityIndex\"],\n",
    "        ),\n",
    "        VectorAssembler(inputCols=[\"cityIndex\", \"fare_amount\", \"speed_mph\"], outputCol=\"features\"),\n",
    "        RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"tip_amount\",\n",
    "            numTrees=80,\n",
    "            maxDepth=10,\n",
    "            minInstancesPerNode=50,\n",
    "            subsamplingRate=0.7,\n",
    "            featureSubsetStrategy=\"sqrt\",\n",
    "            maxBins=64,\n",
    "            seed=42,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model = stages.fit(train)\n",
    "predictions_test = model.transform(test).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c24f72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = model.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de3a50fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF R2: 0.7174909327966459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF MSE: 2.4729636745931662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF MAE: 0.6945249205987529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"tip_amount\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_mse = RegressionEvaluator(labelCol=\"tip_amount\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"tip_amount\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "\n",
    "print(\"RF R2:\", evaluator_r2.evaluate(predictions_test))\n",
    "print(\"RF MSE:\", evaluator_mse.evaluate(predictions_test))\n",
    "print(\"RF MAE:\", evaluator_mae.evaluate(predictions_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f9987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
